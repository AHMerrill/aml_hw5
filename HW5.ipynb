{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoCxNT8Dnaeg"
      },
      "source": [
        "# MIS 382N: Advanced Machine Learning Assignment 5\n",
        "\n",
        "**Total points**: 75 pts\n",
        "\n",
        "**Due**: 11:59 PM CST, Friday, November 21st, 2025.\n",
        "\n",
        "**Submission**:\n",
        "1. Submit your **Jupyter Notebook via Canvas**, AND\n",
        "2. **Save your Jupyter Notebook to a PDF, and submit the PDF via Gradescope**.\n",
        "\n",
        "You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas and Gradescope. But be sure to include the name and UT EID for both students.\n",
        "\n",
        "Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
        "\n",
        "For questions involving mathematical derivations, you can write your answer on paper and then upload an image. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q7oUnKCqpms"
      },
      "source": [
        "**Name(s) and EID(s)**:\n",
        "\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw23xiTndyoR"
      },
      "source": [
        "Bookmarks:\n",
        "\n",
        "Q1. <a href=#Q1>Implementing Logistic Regression</a>\n",
        "\n",
        "Q2. <a href=#Q2>Ensemble Methods for Classification</a>\n",
        "\n",
        "Q3. <a href=#Q3>Ensemble Conceptual Questions</a>\n",
        "\n",
        "\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsAJTZ7n1WQ9"
      },
      "source": [
        "**Q1. Logistic Regression** (30 pts) <a name='Q1'/>\n",
        "\n",
        "In this question, you will implement Logistic Regression from scratch using numpy (not sklearn). You will train the Logistic Regression on a synthetic dataset generated from two Isotropic Gaussians. Then, you will visualize the decision boundary and examine the learned parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3GIHrSa1WQ9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Optional, Tuple\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z9fRJ6t1WQ-"
      },
      "outputs": [],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "n_per_class = 500\n",
        "mu0 = np.array([0.0, 0.0])\n",
        "mu1 = np.array([2.0, 2.0])\n",
        "Sigma = np.array([[1.0, 0.0],\n",
        "                  [0.0, 1.0]])\n",
        "\n",
        "X0 = rng.multivariate_normal(mu0, Sigma, size=n_per_class)\n",
        "X1 = rng.multivariate_normal(mu1, Sigma, size=n_per_class)\n",
        "y0 = np.zeros(n_per_class, dtype=int)\n",
        "y1 = np.ones(n_per_class, dtype=int)\n",
        "\n",
        "X = np.vstack([X0, X1])          # shape (2*n_per_class, 2)\n",
        "y = np.concatenate([y0, y1])     # shape (2*n_per_class,)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train[y_train==0, 0], X_train[y_train==0, 1], s=10, label=\"class 0\")\n",
        "plt.scatter(X_train[y_train==1, 0], X_train[y_train==1, 1], s=10, label=\"class 1\")\n",
        "plt.legend()\n",
        "plt.title(\"Training data: Two Gaussians\")\n",
        "plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOYm_08U1WQ-"
      },
      "source": [
        "**Part 1.** (5 points)\n",
        "\n",
        "We model the probability of a sample belonging to the positive class $p_{\\theta}(y = 1 | \\mathbf{x}) = \\sigma(z)$ with $z = \\mathbf{w}^\\top \\mathbf{x} + b$ and sigmoid $\\sigma(z) = \\frac{1}{1 + \\exp^{-z}}$ where $\\mathbf{x} \\in \\mathbb{R}^d, \\mathbf{w} \\in \\mathbb{R}^d, b \\in \\mathbb{R},$ and $z \\in \\mathbb{R}$. In this exercise, $d=2$.\n",
        "\n",
        "An efficient way to implement Logistic Regression is to compute probabilities of multiple samples simultaneously. Suppose there are $N$ training samples in total, we denote the label vector by $\\mathbf{y} \\in \\{0,1\\}^N$ and the predicted probability vector by $\\mathbf{p} \\in \\mathbb{R}^N$. The $i$'th entries $\\mathbf{y}_i$ and $\\mathbf{p}_i$ corresponds to the label and predicted probability of the $i$'th sample, respectively. Let the data matrix be\n",
        "\n",
        "$$\\mathbf{X} = \\begin{bmatrix}\n",
        "\\mathbf{x}_1^\\top \\\\[4pt]\n",
        "\\mathbf{x}_2^\\top \\\\[4pt]\n",
        "\\vdots \\\\[4pt]\n",
        "\\mathbf{x}_N^\\top\n",
        "\\end{bmatrix} \\in \\mathbb{R}^{N \\times d}$$\n",
        "\n",
        ", the weight vector $\\mathbf{w} \\in \\mathbb{R}^d$ be same as before, and the bias vector be $$\\mathbf{b} = \\begin{bmatrix} b \\\\ \\vdots \\\\ b\\end{bmatrix} \\in \\mathbb{R}^N$$.\n",
        "\n",
        "Then, $\\mathbf{p} = \\sigma(\\mathbf{X}\\mathbf{w} + \\mathbf{b}) \\in \\mathbb{R}^N$ would be the vector of predicted probabilities.\n",
        "\n",
        "\n",
        "\n",
        "The average logistic loss (negative log-likelihood) is:\n",
        "$$L = \\frac{1}{N} \\sum_{i=1}^N [\\mathbf{y}_i \\log \\mathbf{p}_i + (1 - \\mathbf{y}_i) \\log (1 - \\mathbf{p}_i)]$$\n",
        "\n",
        "with optional L2 loss: $\\frac{\\lambda}{2N} \\|\\mathbf{w}\\|_2^2$.\n",
        "\n",
        "Implement the following functions: ```sigmoid```, ```predict_proba```, and ```log_loss```.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmKV_6fl1WQ_"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function.\n",
        "    Args:\n",
        "        z: Input logits. np.ndarray, shape (N,)\n",
        "    Returns\n",
        "        proba: Probabilities after sigmoid transform. np.ndarray, shape (N,)\n",
        "    \"\"\"\n",
        "    # Implement sigmoid function\n",
        "    ### START CODE ###\n",
        "    proba = ...\n",
        "    ### END CODE ###\n",
        "    return proba\n",
        "\n",
        "def predict_proba(X: np.ndarray, w: np.ndarray, b: float) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict class probabilities for inputs X.\n",
        "    Args:\n",
        "        X: Data matrix. np.ndarray, shape (N, d)\n",
        "        w: Weights. np.ndarray, shape (d,)\n",
        "        b: Bias. float\n",
        "    Returns:\n",
        "        proba: Probabilities. np.ndarray, shape (N,)\n",
        "    \"\"\"\n",
        "    # Predict class probabilities for inputs X\n",
        "    ### START CODE ###\n",
        "    proba = ...\n",
        "    ### END CODE ###\n",
        "    return proba\n",
        "\n",
        "def log_loss(y: np.ndarray, p: np.ndarray, l2_lambda: float = 0.0, w: Optional[np.ndarray] = None) -> float:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        y: True labels. np.ndarray, shape (N,)\n",
        "        p: Predicted probabilities. np.ndarray, shape (N,)\n",
        "        l2_lambda: L2 regularization strength. float\n",
        "        w: Weights (for computing L2 regularization loss). np.ndarray, shape (d,)\n",
        "    Returns:\n",
        "        loss: Loss value. float\n",
        "    \"\"\"\n",
        "    # Add a small epsilon to avoid log(0)\n",
        "    eps = 1e-12\n",
        "    p = np.clip(p, eps, 1 - eps)\n",
        "    N = y.shape[0]\n",
        "    # Calculate the log loss\n",
        "    ### START CODE ###\n",
        "    loss = ...\n",
        "    ### END CODE ###\n",
        "\n",
        "    if l2_lambda > 0 and w is not None:\n",
        "        # Add L2 regularization if specified\n",
        "        ### START CODE ###\n",
        "\n",
        "        ### END CODE ###\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHzT687v1WQ_"
      },
      "source": [
        "**Part 2.** (5 points)\n",
        "Given the data matrix $\\mathbf{X}$, the weight vector $\\mathbf{w}$, the bias $b$, the probability vector $\\mathbf{p}$, and the label vector $\\mathbf{y}$, the gradient of the loss is:\n",
        "\n",
        "\\begin{align}\n",
        "\\nabla_{\\mathbf{w}} L &= \\frac{1}{N} \\mathbf{X}^\\top (\\mathbf{p} - \\mathbf{y}) + \\frac{\\lambda}{N} \\mathbf{w} \\in \\mathbb{R}^d \\\\\n",
        "\\nabla_{b} L &= \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{p}_i - \\mathbf{y}_i) \\in \\mathbb{R}\n",
        "\\end{align}\n",
        "\n",
        "Please implement the gradient below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5t2VJ8f1WQ_"
      },
      "outputs": [],
      "source": [
        "def gradients(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    w: np.ndarray,\n",
        "    b: float,\n",
        "    l2_lambda: float = 0.0\n",
        ") -> Tuple[np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Compute gradients of the logistic loss with respect to w and b.\n",
        "    \"\"\"\n",
        "    N = X.shape[0]\n",
        "    p = predict_proba(X, w, b)\n",
        "    # Implement gradient of the logistic loss (w/o regularization component) for the weight vector\n",
        "    ### START CODE ###\n",
        "    grad_w = ...\n",
        "    ### END CODE ###\n",
        "    if l2_lambda > 0:\n",
        "        # Add optional L2 regularization loss gradient\n",
        "        ### START CODE ###\n",
        "\n",
        "        ### END CODE ###\n",
        "    # Implement gradient of the logistic loss for the bias term\n",
        "    ### START CODE ###\n",
        "    grad_b = ...\n",
        "    ### END CODE ###\n",
        "    return grad_w, grad_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkSAkXTO1WQ_"
      },
      "source": [
        "**Part 3.** (5 points) Implement the training function ```train_logreg``` that does full batch gradient descent and track loss per epoch. Then, train a logistic regression model with ```lr=0.25```, ```epochs=300```, ```l2_lambda=0.5``` and visualize the training loss curve.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tK86cuh1WQ_"
      },
      "outputs": [],
      "source": [
        "def train_logreg(X, y, lr=0.1, epochs=300, l2_lambda=0.0):\n",
        "    N, d = X.shape\n",
        "    w = np.zeros(d)\n",
        "    b = 0.0\n",
        "    loss_history = []\n",
        "\n",
        "    for e in range(epochs):\n",
        "        # Get the predicted probabilities\n",
        "        ### START CODE ###\n",
        "        p = ...\n",
        "        ### END CODE ###\n",
        "\n",
        "        # Calculate the loss\n",
        "        ### START CODE ###\n",
        "        loss = ...\n",
        "        ### END CODE ###\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # Calculate the gradients and update the parameters\n",
        "        ### START CODE ###\n",
        "\n",
        "        ### END CODE ###\n",
        "\n",
        "    return w, b, np.array(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_94UDFrc1WQ_"
      },
      "outputs": [],
      "source": [
        "w, b, loss_history = train_logreg(X_train, y_train, lr=0.2, epochs=1000, l2_lambda=0.5)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(loss_history)\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"train loss\")\n",
        "plt.title(\"Training loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkrIolyT1WQ_"
      },
      "source": [
        "**Part 4.** (5 points) Implement the ```predict_label``` function and evaluate the training and testing accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD9SXheP1WQ_"
      },
      "outputs": [],
      "source": [
        "def predict_label(X, w, b, threshold=0.5):\n",
        "    # If the predicted probability >= threshold, predict 1, otherwise predict 0\n",
        "    ### START CODE ###\n",
        "    pred = ...\n",
        "    ### END CODE ###\n",
        "    return pred\n",
        "\n",
        "# Evaluate the training and testing accuracies\n",
        "y_pred_train = predict_label(X_train, w, b)\n",
        "y_pred_test = predict_label(X_test, w, b)\n",
        "\n",
        "train_acc = np.mean(y_pred_train == y_train)\n",
        "test_acc = np.mean(y_pred_test == y_test)\n",
        "print(f\"Training accuracy: {train_acc:.3f}, Testing accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL3EnByq1WRA"
      },
      "source": [
        "Run the following code to visualize the decision boundary and print out the learned Logistic Regression parameters ```w``` and ```b```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGrWkDW91WRA"
      },
      "outputs": [],
      "source": [
        "def plot_boundary(X, y, w, b, title=\"Decision boundary\"):\n",
        "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    pp = predict_proba(grid, w, b).reshape(xx.shape)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.contour(xx, yy, pp, levels=[0.5])\n",
        "    plt.scatter(X[y==0, 0], X[y==0, 1], s=10, label=\"class 0\")\n",
        "    plt.scatter(X[y==1, 0], X[y==1, 1], s=10, label=\"class 1\")\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()\n",
        "\n",
        "plot_boundary(X_train, y_train, w, b, title=\"Train set: learned boundary\")\n",
        "print(f\"Learned parameters: w = {w}, b = {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBHKCl9C1WRA"
      },
      "source": [
        "**Part 5.** (5 points) Are the relative relationship between the learned parameters ($\\mathbf{w}$ and $b$) what you expected according to the synthetic data generation process (Gaussian parameters)? Why?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FIVb5h5bwIAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 6a.** (2.5 points) Suppose one decides to reject if the higher posterior probability is less than 0.7 for a given $\\mathbf{x}$. **Derive** the reject region in the input space by identifying the two boundaries of this region (provide two inequalities) and then complete the code below to plot the region.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "T4CxGs-AJCmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WaglmdDNJSlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_rejection_boundaries(X, y, w, b, p1, p2, title=\"Rejection boundary\"):\n",
        "    \"\"\"\n",
        "    The new args p1 and p2 are the probabilities of the rejection boundaries. p1 < p2.\n",
        "    \"\"\"\n",
        "    assert p1 <= p2\n",
        "    x_min, x_max = X[:,0].min()-1, X[:,0].max()+1\n",
        "    y_min, y_max = X[:,1].min()-1, X[:,1].max()+1\n",
        "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
        "                         np.linspace(y_min, y_max, 200))\n",
        "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "    pp = predict_proba(grid, w, b).reshape(xx.shape)\n",
        "\n",
        "    plt.figure()\n",
        "    # Plot the rejection boundaries and the shaded region in between\n",
        "    ### START CODE ###\n",
        "    plt.contour(xx, yy, pp, levels=...)\n",
        "    plt.contour(xx, yy, pp, levels=...)\n",
        "    plt.contourf(xx, yy, pp, levels=..., colors=['tab:blue'], alpha=0.2, zorder=0)\n",
        "    ### END CODE ###\n",
        "    plt.scatter(X[y==0, 0], X[y==0, 1], s=10, label=\"class 0\")\n",
        "    plt.scatter(X[y==1, 0], X[y==1, 1], s=10, label=\"class 1\")\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"x1\"); plt.ylabel(\"x2\")\n",
        "    plt.show()\n",
        "\n",
        "plot_rejection_boundaries(X_train, y_train, w, b, 0.3, 0.7, title=\"Train set: learned boundary\")\n",
        "print(f\"Learned parameters: w = {w}, b = {b}\")"
      ],
      "metadata": {
        "id": "IY4NGjT6ebiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 6b.** (2.5 points) State in plain English the impact of independent variable $x_1$  on a \"determination\" for Class 1, by properly interpreting its coefficient returned by your logistic regression model.\n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "8GO8uPURMXkg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByHwdCPb1WRA"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc9ca1b5"
      },
      "source": [
        "**Q2. Ensemble Methods for Classification** (35 pts) <a name='Q2'/>\n",
        "\n",
        "In this question, we will compare the performances of [Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and different ensemble methods for classification problems: [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html), [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), [Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) classifiers.\n",
        "\n",
        "We will look at the [GiveMeSomeCredit](https://www.kaggle.com/c/GiveMeSomeCredit) dataset for this question. The dataset is extremely large so for this question we will only consider a subset which has been provided along with the notebook for this assignment. The dataset has already been split into train and test sets.\n",
        "\n",
        "The task is to predict the probability that someone will experience financial distress in the next two years.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a93b913f"
      },
      "outputs": [],
      "source": [
        "# Only use this code block if you are using Google Colab.\n",
        "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
        "from google.colab import files\n",
        "\n",
        "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n",
        "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fjd8DQ-U1WRA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import xgboost\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a2bac93"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('hw5_data.csv')\n",
        "data.drop(data.columns[data.columns.str.contains('unnamed',case = False)],axis=1, inplace=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb34060d"
      },
      "outputs": [],
      "source": [
        "y = data['SeriousDlqin2yrs']\n",
        "X = data.drop(['SeriousDlqin2yrs'], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print('train:',X_train.shape, y_train.shape)\n",
        "print('test:',X_test.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "420aeb16"
      },
      "outputs": [],
      "source": [
        "columns_list = list(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWpJLf-x1WRA"
      },
      "source": [
        "**Part 1.** (5 points) Complete the following functions that will be repeatedly used later: ```evaluate_classifier```, ```train_and_evaluate_classifier```, and ```grid_search_for_classifier```. Please implement according to the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrLgBD84yFHy"
      },
      "outputs": [],
      "source": [
        "def evaluate_classifier(clf, X_eval, y_eval):\n",
        "    # Perform prediction on X_eval and extract the probability score of the positive class\n",
        "    ### START CODE ###\n",
        "    y_pred = ...\n",
        "    y_pred_proba = ...\n",
        "    ### END CODE ###\n",
        "\n",
        "    # Calculate accuracy and AU-ROC score\n",
        "    ### START CODE ###\n",
        "    acc_score = ...\n",
        "    auc_score = ...\n",
        "    ### END CODE ###\n",
        "\n",
        "    return acc_score, auc_score\n",
        "\n",
        "def train_and_evaluate_classifier(clf, X_train, y_train, X_eval, y_eval):\n",
        "    # Fit your classifier on the training set\n",
        "    ### START CODE ###\n",
        "\n",
        "    ### END CODE ###\n",
        "\n",
        "    acc_score, auc_score = evaluate_classifier(clf, X_eval, y_eval)\n",
        "    return clf, acc_score, auc_score\n",
        "\n",
        "def grid_search_for_classifier(clf, param_grid, X_train, y_train):\n",
        "    # Initialize GridSearchCV. Use 5-fold cross validation and AU-ROC as the scoring metric.\n",
        "    ### START CODE ###\n",
        "    grid_search = ...\n",
        "    ### END CODE ###\n",
        "\n",
        "    # Perform grid search\n",
        "    ### START CODE ###\n",
        "\n",
        "    ### END CODE ###\n",
        "\n",
        "    return grid_search.best_estimator_, grid_search.best_params_, grid_search.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f40f9107"
      },
      "source": [
        "**Part 2.** (5 points) Fit a Decision Tree Classifier with ```random_state=42``` for this classification problem. Tune the following hyper-parameters: ```max_depth```, ```min_samples_split```, and ```min_samples_leaf```. Report the ```accuracy_score``` and ```roc_auc_score``` on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYUSC5gH1WRB"
      },
      "outputs": [],
      "source": [
        "# Define your hyper-parameter grid for decision tree classifier\n",
        "### START CODE ###\n",
        "hparams_grid_dt = {\n",
        "    ...\n",
        "}\n",
        "### END CODE ###\n",
        "clf_dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search and get the trained classifier, best hyper-parameters, and best AU-ROC score\n",
        "### START CODE ###\n",
        "clf_dt, best_hparams_dt, best_score_dt = ...\n",
        "### END CODE ###\n",
        "print(f\"Best Hyper-parameters: {best_hparams_dt}, AU-ROC: {best_score_dt:.3f}\")\n",
        "\n",
        "# Evaluate the decision tree classifier on the test set\n",
        "### START CODE ###\n",
        "test_acc_dt, test_auc_dt = ...\n",
        "### END CODE ###\n",
        "print(f\" Decision Tree Test Accuracy: {test_acc_dt:.3f}, Test AU-ROC: {test_auc_dt:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d360a99"
      },
      "source": [
        "**Part 3.** (5 points) Create a [Bagging](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) of 10 classifiers (i.e, n_estimators=10) with ```random_state=42```. Please use Decision Tree Classifier with ```random_state=42``` and the previously found best hyper-parameter combination as the base classifier. Report ```accuracy_score``` and ```roc_auc_score``` on the test data for this emsemble classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "239c9c3c"
      },
      "outputs": [],
      "source": [
        "# Initialize your bagging classifier\n",
        "### START CODE ###\n",
        "clf_bag = ...\n",
        "### END CODE ###\n",
        "\n",
        "# Train and evaluate the bagging classifier\n",
        "### START CODE ###\n",
        "clf_bag, test_acc_bag, test_auc_bag = ...\n",
        "### END CODE ###\n",
        "print(f\"Bagging of Decision Trees Test Accuracy: {test_acc_bag:.3f}, Test AU-ROC: {test_auc_bag:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34225ee3"
      },
      "source": [
        "**Part 4.** (5 points) In this question, you will fit a [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) model on the training data for this classification task.\n",
        "\n",
        "1. First, please set ```random_state=42```, find the best hyper-parameters, and refit your classifier with those hyper-parameters. Consider tuning the following hyper-parameters: ```n_estimators```, ```max_depth```, ```min_samples_leaf```, and ```bootstrap```.\n",
        "2. Second, evaluate your fitted random forest classifier on the test set. Report the accuracy and AU-ROC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bfb9542"
      },
      "outputs": [],
      "source": [
        "# Define your hyper-parameter grid for random forest classifier\n",
        "### START CODE ###\n",
        "hparams_grid_rf = {\n",
        "    ...\n",
        "}\n",
        "### END CODE ###\n",
        "\n",
        "# Initialize your random forest classifier\n",
        "### START CODE ###\n",
        "clf_rf = ...\n",
        "### END CODE ###\n",
        "\n",
        "# Perform grid search and get the trained classifier, best hyper-parameters, and best AU-ROC score\n",
        "### START CODE ###\n",
        "clf_rf, best_hparams_rf, best_score_rf = ...\n",
        "### END CODE ###\n",
        "print(f\"Best Hyper-parameters: {best_hparams_rf}, AU-ROC: {best_score_rf:.3f}\")\n",
        "\n",
        "# Evaluate the random forest classifier on the test set\n",
        "### START CODE ###\n",
        "test_acc_rf, test_auc_rf = ...\n",
        "### END CODE ###\n",
        "print(f\"Random Forest Test Accuracy: {test_acc_rf:.3f}, Test AU-ROC: {test_auc_rf:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "314c313b"
      },
      "source": [
        "**Part 5.** (10 points) This time, let us use [AdaBoost](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) and [XGBoost](https://xgboost.readthedocs.io/en/stable/python/python_api.html) for the same task. For AdaBoost and XGBoost, please set ```random_state=42``` and find the best hyper-parameters such as ```n_estimators```, ```learning_rate```, ```max_depth```. Refit your model using the best hyper-parameters, and report the ```accuracy_score``` and ```roc_auc_score``` on test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bp4Ekjpy1cGw"
      },
      "outputs": [],
      "source": [
        "# Define your hyper-parameter grid for AdaBoost classifier\n",
        "### START CODE ###\n",
        "hparams_grid_ab = {\n",
        "    ...\n",
        "}\n",
        "### END CODE ###\n",
        "\n",
        "# Initialize your AdaBoost classifier\n",
        "### START CODE ###\n",
        "clf_ab = ...\n",
        "### END CODE ###\n",
        "\n",
        "# Perform grid search and get the trained classifier, best hyper-parameters, and best AU-ROC score\n",
        "### START CODE ###\n",
        "clf_ab, best_hparams_ab, best_score_ab = ...\n",
        "### END CODE ###\n",
        "print(f\"Best Hyper-parameters: {best_hparams_ab}, AU-ROC: {best_score_ab:.3f}\")\n",
        "\n",
        "# Evaluate the AdaBoost classifier on the test set\n",
        "### START CODE ###\n",
        "test_acc_ab, test_auc_ab = ...\n",
        "print(f\" AdaBoost Test Accuracy: {test_acc_ab:.3f}, Test AU-ROC: {test_auc_ab:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv0p1mZh2CqA"
      },
      "outputs": [],
      "source": [
        "# Define your hyper-parameter grid for XGBoost classifier\n",
        "### START CODE ###\n",
        "hparams_grid_xgb = {\n",
        "    ...\n",
        "}\n",
        "### END CODE ###\n",
        "# Initialize your XGBoost classifier\n",
        "### START CODE ###\n",
        "clf_xgb = ...\n",
        "### END CODE ###\n",
        "\n",
        "# Perform grid search and get the trained classifier, best hyper-parameters, and best AU-ROC score\n",
        "### START CODE ###\n",
        "clf_xgb, best_hparams_xgb, best_score_xgb = ...\n",
        "### END CODE ###\n",
        "print(f\"Best Hyper-parameters: {best_hparams_xgb}, AU-ROC: {best_score_xgb:.3f}\")\n",
        "\n",
        "# Evaluate the XGBoost classifier on the test set\n",
        "### START CODE ###\n",
        "test_acc_xgb, test_auc_xgb = ...\n",
        "### END CODE ###\n",
        "print(f\" XGBoost Test Accuracy: {test_acc_xgb:.3f}, Test AU-ROC: {test_auc_xgb:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6bc87cd"
      },
      "source": [
        "**Part 6.** (5 points) Compare the performance of decision tree with the ensemble methods that you tried. Please briefly describe the key concepts behind the ensemble method that performed the best for this dataset.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ib866Hc_v26a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPWsGQB0MnH4"
      },
      "source": [
        "**Q3. Ensemble Conceptual Questions** (10 pts) <a name='Q3'/>\n",
        "\n",
        "**Part 1.** (5 points) Boosting combines a series of weak predictors into a strong predictor. However, it has the danger of being sensitive to outliers. Please use XGBoost as an example to briefly describe how outliers may undermine the algorithm.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TCdNrLv-v4wn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k89_6Z8F1WRG"
      },
      "source": [
        "**Part 2.** (5 points) Where does the randomness of random forests come from?\n",
        "\n",
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v0Y7N4FYv6Ej"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}